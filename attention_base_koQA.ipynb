{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch.utils.data as data\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "import copy\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "qa_dataset = load_dataset(\"maywell/ko_wikidata_QA\", cache_dir=\"../../../data/jeongseokoh/dataset/\")\n",
    "tokenizer_ko = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# Train 데이터셋을 랜덤하게 섞습니다.\n",
    "shuffled_dataset = qa_dataset['train'].shuffle(seed=42)\n",
    "\n",
    "# 30,000개를 train용, 10,000개를 validation용으로 분리\n",
    "train_dataset = shuffled_dataset.select(range(30000))\n",
    "validation_dataset = shuffled_dataset.select(range(30000, 40000))\n",
    "\n",
    "\n",
    "def ko_tokenize_function(examples):\n",
    "    # 영어(en)와 독일어(de) 텍스트에 대한 토크나이징 수행\n",
    "    tokenized_inputs = tokenizer_ko(examples[\"instruction\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # 타겟 토크나이저로 영어 텍스트 토크나이징\n",
    "    with tokenizer_ko.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer_ko(examples[\"output\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    \n",
    "    # 모델 입력을 위한 tokenized_inputs 반환 및 레이블(labels)로 tokenized_targets[\"input_ids\"] 설정\n",
    "    tokenized_inputs[\"labels\"] = tokenized_targets[\"input_ids\"]\n",
    "    return tokenized_inputs\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "tokenized_datasets = shuffled_dataset.map(ko_tokenize_function, batched=True)\n",
    "val_tokenized_datasets = validation_dataset.map(ko_tokenize_function, batched=True)\n",
    "\n",
    "# DataLoader 재정의\n",
    "dataloader = DataLoader(tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(val_tokenized_datasets, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        #print(f\"d_model : {d_model}\")\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        #print(f\"Q: {Q.shape}\")\n",
    "\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        #print(Q.shape)\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(f\"Output: {output.shape}\")\n",
    "        return output\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print((x + self.pe[:, :x.size(1)]).shape)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "## 1. CustomWeightLayer를 expand와 shrink로 나눈다. \n",
    "## 2-1. 768차원으로 늘려서 Encoder의 Self-attention에 넣는다.\n",
    "## 2-2. 768차원으로 늘려서 바로 decoder의 Cross-attention에 넣는다. \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, tokens):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        our_output = self.self_attn(x, x, x, mask)\n",
    "        \n",
    "        x = self.norm1(x + self.dropout(our_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, tokens):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        seq_length = tgt.size(1)\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for i, enc_layer in enumerate(self.encoder_layers):\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluke0112\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/jeongseokoh/jeongseokoh/bnn/wandb/run-20240413_182014-coqqtild</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luke0112/Sparse_plane/runs/coqqtild' target=\"_blank\">Base_ko</a></strong> to <a href='https://wandb.ai/luke0112/Sparse_plane' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luke0112/Sparse_plane' target=\"_blank\">https://wandb.ai/luke0112/Sparse_plane</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luke0112/Sparse_plane/runs/coqqtild' target=\"_blank\">https://wandb.ai/luke0112/Sparse_plane/runs/coqqtild</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = tokenizer_ko.vocab_size\n",
    "tgt_vocab_size = tokenizer_ko.vocab_size\n",
    "d_model = 768  # BERT의 경우\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 256\n",
    "dropout = 0.1\n",
    "wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Sparse_plane\",\n",
    "        name=\"Base_ko\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"architecture\": \"Scratch\",\n",
    "            \"dataset\": \"de,en\",\n",
    "            \"epochs\": 10,\n",
    "            }\n",
    "    )\n",
    "# 모델 초기화\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "#model_state_dict = torch.load('../../../data/jeongseokoh/model/base_noise_de_en.pth')\n",
    "#transformer.load_state_dict(model_state_dict)\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_ko.pad_token_id)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# 모델 학습\n",
    "transformer.train()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 11):  # 에폭 수 조정 가능\n",
    "    total_loss = 0\n",
    "    total_var = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Train Iteration\"):\n",
    "        src = batch['input_ids'].to(device)  # 장치로 이동\n",
    "        tgt = batch['labels'].to(device)     # 장치로 이동\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = transformer(src, tgt[:, :-1])  # [32, 128, vocab_size], [32]\n",
    "        \n",
    "        # 교차 엔트로피 손실 계산\n",
    "        # 교차 엔트로피 손실 계산\n",
    "        loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"Loss\": loss.item()})\n",
    "        total_loss += loss.item()\n",
    "        #total_var += variances.mean()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    #avg_var = total_var / len(dataloader)\n",
    "    print(f\"Epoch: {epoch}, Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        # You can add more components here as needed.\n",
    "    }\n",
    "    torch.save(checkpoint, f'../../parameters/scratch/base_attention_ko_wiki_checkpoint_epoch_{epoch}.pth')\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(val_dataloader, desc=\"Val Iteration\"):\n",
    "        src = batch['input_ids'].to(device)  # 장치로 이동\n",
    "        tgt = batch['labels'].to(device)     # 장치로 이동\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = transformer(src, tgt[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))  \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    print(f\"Epoch: {epoch}, Avg Validation Loss: {avg_loss:.4f}\")\n",
    "    wandb.log({\"Val_Loss\": avg_loss})  \n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"총 시간: {end_time - start_time} sec\")\n",
    "torch.save(transformer.state_dict(), '../../parameters/scratch/base_attention_ko_wiki.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = tokenizer_ko.vocab_size\n",
    "tgt_vocab_size = tokenizer_ko.vocab_size\n",
    "d_model = 768  # BERT의 경우\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "# 모델 초기화\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "model_state_dict = torch.load('../../parameters/scratch/base_noise_de_en_checkpoint_epoch_8.pth')\n",
    "transformer.load_state_dict(model_state_dict['model_state_dict'])\n",
    "# 모델 초기화\n",
    "#transformer2 = Transformer(\n",
    "#    src_vocab_size=src_vocab_size,\n",
    "#    tgt_vocab_size=tgt_vocab_size,\n",
    "#    d_model=d_model,\n",
    "#    num_heads=num_heads,\n",
    "#    num_layers=num_layers,\n",
    "#    d_ff=d_ff,\n",
    "#    max_seq_length=max_seq_length,\n",
    "#    dropout=dropout\n",
    "#).to(device)\n",
    "#model_state_dict = torch.load('../../parameters/scratch/base_de_en_checkpoint_epoch_7.pth')\n",
    "#transformer2.load_state_dict(model_state_dict['model_state_dict'])\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.pad_token_id)\n",
    "def predict(model, input_sequence, max_length=15, SOS_token=101, EOS_token=102):\n",
    "    model.eval()\n",
    "    # Ensure input_sequence is a tensor and reshape if necessary\n",
    "    if not isinstance(input_sequence, torch.Tensor):\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.long)\n",
    "    input_sequence = input_sequence.unsqueeze(0)  # Add batch dimension if missing\n",
    "    input_sequence = input_sequence.to(device)  # Move to the same device as the model\n",
    "\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            pred, variances = model(input_sequence, y_input)\n",
    "            next_item = pred[:, -1, :].topk(1)[1].view(-1).item()  # Get the last word's top prediction\n",
    "            next_item = torch.tensor([[next_item]], device=device)\n",
    "            #print(next_item)\n",
    "            y_input = torch.cat((y_input, next_item), dim=1)\n",
    "            if next_item.view(-1).item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return y_input.view(-1).tolist(), variances\n",
    " \n",
    "val_dataloader.dataset['labels'][1]\n",
    "# Here we test some examples to observe how the model predicts\n",
    "\n",
    "examples2 = test_dataloader.dataset['input_ids'][:10]\n",
    " \n",
    "for idx, example in enumerate(examples2):\n",
    "    result, variances = predict(transformer, example)\n",
    "    result2, variances2 = predict(transformer2, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example[1:-1]}\")\n",
    "    print(f\"GoldenAnswer: {test_dataloader.dataset['en'][idx]}\")\n",
    "    print(f\"SparseAnswer: {tokenizer_ko.decode(result[1:])}\")\n",
    "    print(f\"Base  Answer: {tokenizer_ko.decode(result2[1:])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "env01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
