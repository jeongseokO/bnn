{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch.utils.data as data\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "import copy\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "qa_dataset = load_dataset(\"maywell/ko_wikidata_QA\", cache_dir=\"../../../data/jeongseokoh/dataset/\")\n",
    "tokenizer_ko = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# Train 데이터셋을 랜덤하게 섞습니다.\n",
    "shuffled_dataset = qa_dataset['train'].shuffle(seed=42)\n",
    "\n",
    "# 30,000개를 train용, 10,000개를 validation용으로 분리\n",
    "#train_dataset = shuffled_dataset.select(range(30000))\n",
    "validation_dataset = shuffled_dataset.select(range(30000, 40000))\n",
    "\n",
    "\n",
    "def ko_tokenize_function(examples):\n",
    "    # 영어(en)와 독일어(de) 텍스트에 대한 토크나이징 수행\n",
    "    tokenized_inputs = tokenizer_ko(examples[\"instruction\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # 타겟 토크나이저로 영어 텍스트 토크나이징\n",
    "    with tokenizer_ko.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer_ko(examples[\"output\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    \n",
    "    # 모델 입력을 위한 tokenized_inputs 반환 및 레이블(labels)로 tokenized_targets[\"input_ids\"] 설정\n",
    "    tokenized_inputs[\"labels\"] = tokenized_targets[\"input_ids\"]\n",
    "    return tokenized_inputs\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "tokenized_datasets = shuffled_dataset.map(ko_tokenize_function, batched=True)\n",
    "val_tokenized_datasets = validation_dataset.map(ko_tokenize_function, batched=True)\n",
    "\n",
    "# DataLoader 재정의\n",
    "dataloader = DataLoader(tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(val_tokenized_datasets, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        #print(f\"d_model : {d_model}\")\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        #print(f\"Q: {Q.shape}\")\n",
    "\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        #print(Q.shape)\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(f\"Output: {output.shape}\")\n",
    "        return output\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print((x + self.pe[:, :x.size(1)]).shape)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "## 1. CustomWeightLayer를 expand와 shrink로 나눈다. \n",
    "## 2-1. 768차원으로 늘려서 Encoder의 Self-attention에 넣는다.\n",
    "## 2-2. 768차원으로 늘려서 바로 decoder의 Cross-attention에 넣는다. \n",
    "class CustomWeightLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, tokens):\n",
    "        super(CustomWeightLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tokens = tokens\n",
    "        self.out_dim1 = 3 * self.tokens - 1\n",
    "        self.out_dim2 = self.d_model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 가중치 및 바이어스 초기화를 더 효율적으로 수행\n",
    "        self.expand_w = nn.Parameter(torch.randn(self.out_dim1, self.tokens) * 0.01)\n",
    "        self.expand_bias = nn.Parameter(torch.zeros(self.out_dim1))\n",
    "        self.feature1_w = nn.Parameter(torch.randn(self.out_dim2, self.d_model) * 0.01)\n",
    "        self.feature1_bias = nn.Parameter(torch.zeros(self.out_dim2))\n",
    "        self.shrink_w = nn.Parameter(torch.randn(self.tokens, self.out_dim1) * 0.01)\n",
    "        self.shrink_bias = nn.Parameter(torch.zeros(self.tokens))\n",
    "        self.feature2_w = nn.Parameter(torch.randn(self.d_model, self.out_dim2) * 0.01)\n",
    "        self.feature2_bias = nn.Parameter(torch.zeros(self.d_model))\n",
    "        # 가중치에서 특정 패턴을 0으로 설정\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # expand_w와 shrink_w의 특정 패턴을 0으로 초기화\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.tokens):\n",
    "                for j in range(self.tokens - 1):\n",
    "                    self.expand_w.data[i - self.tokens + 1 + j, i] = 0.0\n",
    "                    self.shrink_w.data[i, i - self.tokens + 1 + j] = 0.0\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.squeeze(1).squeeze(1)  # 두 번째와 세 번째 차원을 제거\n",
    "            mask = mask.unsqueeze(-1)  # 마지막 차원에 차원 추가\n",
    "            x = x * mask.float()\n",
    "        # 텐서 조작을 최소화하고, 불필요한 전치를 피함\n",
    "        x = F.linear(x, self.feature1_w, self.feature1_bias)\n",
    "        x = F.linear(x.transpose(1, 2), self.expand_w, self.expand_bias)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = F.linear(x, self.shrink_w, self.shrink_bias).transpose(1, 2)\n",
    "        x = F.linear(x, self.feature2_w, self.feature2_bias)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, tokens):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.custom = CustomWeightLayer(d_model, num_heads, self.tokens)\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask, last_layer=False):\n",
    "        our_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(our_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, tokens):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        #self.custom = CustomWeightLayer(d_model, num_heads, self.tokens-1)\n",
    "        #self.adaptive_pool = nn.AdaptiveAvgPool1d(127)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        #print(f\"x.size: {x.shape}\")\n",
    "        enc_output =  torch.cat((enc_output, x), dim=1)\n",
    "        #print(f\"enc_output.size: {enc_output.shape}\")\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        #print(f\"attn_output.size: {attn_output.shape}\")\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        #print(tgt_mask.shape)\n",
    "        #print(src_mask.shape)\n",
    "        # src_mask를 tgt_mask 크기에 맞게 확장\n",
    "        src_mask_expanded = src_mask.expand(-1, -1, seq_length, -1)  # 마지막 차원을 유지하면서 세 번째 차원을 seq_length로 확장\n",
    "\n",
    "        # 두 텐서를 마지막 차원을 기준으로 연결\n",
    "        combined_mask = torch.cat((src_mask_expanded, tgt_mask), dim=3)  # 마지막 차원(3)을 기준으로 연결\n",
    "\n",
    "        return src_mask, tgt_mask, combined_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        seq_length = tgt.size(1)\n",
    "        src_mask, tgt_mask, combined_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        last_layer = False\n",
    "        for i, enc_layer in enumerate(self.encoder_layers):\n",
    "            if i+1 == self.num_layers:\n",
    "                last_layer = True\n",
    "            else:\n",
    "                last_layer = False\n",
    "            enc_output = enc_layer(enc_output, src_mask, last_layer)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, combined_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluke0112\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/jeongseokoh/jeongseokoh/bnn/wandb/run-20240413_181700-v425piwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luke0112/Sparse_plane/runs/v425piwn' target=\"_blank\">hopeful-frog-34</a></strong> to <a href='https://wandb.ai/luke0112/Sparse_plane' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luke0112/Sparse_plane' target=\"_blank\">https://wandb.ai/luke0112/Sparse_plane</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luke0112/Sparse_plane/runs/v425piwn' target=\"_blank\">https://wandb.ai/luke0112/Sparse_plane/runs/v425piwn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Iteration: 100%|██████████| 4298/4298 [35:18<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Train Loss: 4.5080\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = tokenizer_ko.vocab_size\n",
    "tgt_vocab_size = tokenizer_ko.vocab_size\n",
    "d_model = 768  # BERT의 경우\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 256\n",
    "dropout = 0.1\n",
    "wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Sparse_plane\",\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"architecture\": \"Scratch\",\n",
    "            \"dataset\": \"de,en\",\n",
    "            \"epochs\": 10,\n",
    "            }\n",
    "    )\n",
    "# 모델 초기화\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "#model_state_dict = torch.load('../../../data/jeongseokoh/model/base_noise_de_en.pth')\n",
    "#transformer.load_state_dict(model_state_dict)\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_ko.pad_token_id)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# 모델 학습\n",
    "transformer.train()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 11):  # 에폭 수 조정 가능\n",
    "    total_loss = 0\n",
    "    total_var = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Train Iteration\"):\n",
    "        src = batch['input_ids'].to(device)  # 장치로 이동\n",
    "        tgt = batch['labels'].to(device)     # 장치로 이동\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = transformer(src, tgt[:, :-1])  # [32, 128, vocab_size], [32]\n",
    "        \n",
    "        # 교차 엔트로피 손실 계산\n",
    "        # 교차 엔트로피 손실 계산\n",
    "        basic_loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "        # 분산의 영향을 조절하기 위한 가중치 인자\n",
    "        #variance_weight = 0.01  # 이 값은 실험을 통해 최적화해야 함\n",
    "\n",
    "        # 손실에서 분산의 가중치를 곱한 값을 빼서 새로운 손실 값을 계산\n",
    "        # 여기서 max 함수를 사용하여 손실 값이 음수가 되지 않도록 합니다.\n",
    "        #adjusted_loss = basic_loss + torch.max(0.2 - variances.mean(), torch.tensor(0.0))\n",
    "\n",
    "        basic_loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"Loss\": basic_loss.item()})\n",
    "        total_loss += basic_loss.item()\n",
    "        #total_var += variances.mean()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    #avg_var = total_var / len(dataloader)\n",
    "    print(f\"Epoch: {epoch}, Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        # You can add more components here as needed.\n",
    "    }\n",
    "    torch.save(checkpoint, f'../../parameters/scratch/expand_att_ko_wiki_checkpoint_epoch_{epoch}.pth')\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(val_dataloader, desc=\"Val Iteration\"):\n",
    "        src = batch['input_ids'].to(device)  # 장치로 이동\n",
    "        tgt = batch['labels'].to(device)     # 장치로 이동\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = transformer(src, tgt[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
    "         \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    print(f\"Epoch: {epoch}, Avg Validation Loss: {avg_loss:.4f}\")\n",
    "    wandb.log({\"Val_Loss\": avg_loss})   \n",
    "end_time = time.time()\n",
    "print(f\"총 시간: {end_time - start_time} sec\")\n",
    "torch.save(transformer.state_dict(), '../../parameters/scratch/expand_att_ko_wiki.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 129, 768])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 정의\n",
    "tensor1 = torch.randn(32, 128, 768)\n",
    "tensor2 = torch.randn(32, 1, 768)\n",
    "\n",
    "# 두 텐서를 두 번째 차원을 기준으로 연결\n",
    "result_tensor = torch.cat((tensor1, tensor2), dim=1)\n",
    "\n",
    "#print(result_tensor)\n",
    "print(result_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [917, 2509, 88, 9712, 26280, 3841, 20640, 26897, 115, 303, 19630, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A group of men are loading cotton onto a truck\n",
      "ExpandAnswer: a group of men are unloading a truck. [SEP]\n",
      "\n",
      "Example 1\n",
      "Input: [198, 980, 10640, 136, 50, 297, 15544, 2566, 115, 297, 507, 8716, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A man sleeping in a green room on a couch.\n",
      "ExpandAnswer: a man sleeping on a couch in a green room. [SEP]\n",
      "\n",
      "Example 2\n",
      "Input: [198, 10828, 114, 3506, 20973, 26898, 8268, 115, 86, 13684, 26898, 225, 946, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A boy wearing headphones sits on a woman's shoulders.\n",
      "ExpandAnswer: a boy with headphones sitting on his shoulder. [SEP]\n",
      "\n",
      "Example 3\n",
      "Input: [1153, 3284, 9695, 155, 24940, 1703, 20656, 26900, 15086, 115, 297, 4125, 2789, 26838, 3191, 115, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: Two men setting up a blue ice fishing hut on an iced over lake\n",
      "ExpandAnswer: two men building a blue ice cream cone on a lake. [SEP]\n",
      "\n",
      "Example 4\n",
      "Input: [198, 980, 114, 22985, 6, 23778, 15065, 26897, 26918, 21, 155, 14737, 8521, 2800, 26897, 3266, 26918, 8268, 50, 297, 3032, 10494, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A balding man wearing a red life jacket is sitting in a small boat.\n",
      "ExpandAnswer: a balding man wearing a red life vest sits in a small boat. [SEP]\n",
      "\n",
      "Example 5\n",
      "Input: [917, 946, 50, 297, 14737, 26911, 22828, 26918, 30, 155, 5146, 147, 9959, 16739, 1364, 4777, 1065, 50, 297, 18903, 8993, 4429, 3330, 26918, 15284, 2768, 142, 303, 3103, 8670, 2534, 50, 30, 2116, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
      "ExpandAnswer: a woman in a red coat holding a leaf handbag is jumping in a blue plastic bagel bag. [SEP]\n",
      "\n",
      "Example 6\n",
      "Input: [198, 20869, 6, 9217, 8834, 4053, 128, 10400, 9217, 18485, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A brown dog is running after the black dog.\n",
      "ExpandAnswer: a brown dog is running after a black dog. [SEP]\n",
      "\n",
      "Example 7\n",
      "Input: [198, 5934, 10828, 114, 297, 3945, 15288, 26935, 22321, 1789, 2768, 303, 22829, 5391, 26900, 50, 3134, 443, 24654, 65, 4106, 26902, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
      "ExpandAnswer: a little boy in a team uniform swings a baseball bat at a pitch. [SEP]\n",
      "\n",
      "Example 8\n",
      "Input: [198, 980, 15826, 321, 50, 297, 16429, 139, 7949, 7, 7173, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A man in a cluttered office is using the telephone\n",
      "ExpandAnswer: a man on a jungle office setting in a jungle office setting. [SEP]\n",
      "\n",
      "Example 9\n",
      "Input: [917, 1567, 343, 26907, 21748, 946, 114, 297, 6699, 24162, 20, 9790, 7, 6129, 3653, 9614, 3330, 39, 20329, 5475, 772, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A smiling woman in a peach tank top stands holding a mountain bike\n",
      "ExpandAnswer: a smiling woman with a peach - sleeve tank top holding a mountain bike. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model, input_sequence, max_length=128, SOS_token=101, EOS_token=102):\n",
    "    model.eval()\n",
    "    # Ensure input_sequence is a tensor and reshape if necessary\n",
    "    if not isinstance(input_sequence, torch.Tensor):\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.long)\n",
    "    input_sequence = input_sequence.unsqueeze(0)  # Add batch dimension if missing\n",
    "    input_sequence = input_sequence.to(device)  # Move to the same device as the model\n",
    "\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            pred = model(input_sequence, y_input)\n",
    "            next_item = pred[:, -1, :].topk(1)[1].view(-1).item()  # Get the last word's top prediction\n",
    "            next_item = torch.tensor([[next_item]], device=device)\n",
    "            #print(next_item)\n",
    "            y_input = torch.cat((y_input, next_item), dim=1)\n",
    "            if next_item.view(-1).item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    " \n",
    "val_dataloader.dataset['labels'][1]\n",
    "# Here we test some examples to observe how the model predicts\n",
    "\n",
    "examples2 = test_dataloader.dataset['input_ids'][:10]\n",
    " \n",
    "for idx, example in enumerate(examples2):\n",
    "    result = predict(transformer, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example[1:-1]}\")\n",
    "    print(f\"GoldenAnswer: {test_dataloader.dataset['en'][idx]}\")\n",
    "    print(f\"ExpandAnswer: {tokenizer_ko.decode(result[1:])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [917, 2509, 88, 9712, 26280, 3841, 20640, 26897, 115, 303, 19630, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A group of men are loading cotton onto a truck\n",
      "ExpandAnswer: a group of men are putting tree on a truck. [SEP]\n",
      "Base  Answer: a group of men are loading tree stump onto a truck. [SEP]\n",
      "\n",
      "Example 1\n",
      "Input: [198, 980, 10640, 136, 50, 297, 15544, 2566, 115, 297, 507, 8716, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A man sleeping in a green room on a couch.\n",
      "ExpandAnswer: a man sleeping in a green room on a couch. [SEP]\n",
      "Base  Answer: a man sleeping on a couch in a green room. [SEP]\n",
      "\n",
      "Example 2\n",
      "Input: [198, 10828, 114, 3506, 20973, 26898, 8268, 115, 86, 13684, 26898, 225, 946, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A boy wearing headphones sits on a woman's shoulders.\n",
      "ExpandAnswer: a boy wearing headphones sitting on his shoulders. [SEP]\n",
      "Base  Answer: a boy with headphones sitting on the shoulders of a woman. [SEP]\n",
      "\n",
      "Example 3\n",
      "Input: [1153, 3284, 9695, 155, 24940, 1703, 20656, 26900, 15086, 115, 297, 4125, 2789, 26838, 3191, 115, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: Two men setting up a blue ice fishing hut on an iced over lake\n",
      "ExpandAnswer: two men are building a blue ice cream cone on a sunny day. [SEP]\n",
      "Base  Answer: two men are building a blue ice cream cone on a lake. [SEP]\n",
      "\n",
      "Example 4\n",
      "Input: [198, 980, 114, 22985, 6, 23778, 15065, 26897, 26918, 21, 155, 14737, 8521, 2800, 26897, 3266, 26918, 8268, 50, 297, 3032, 10494, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A balding man wearing a red life jacket is sitting in a small boat.\n",
      "ExpandAnswer: a balding man wearing a red life jacket is sitting in a small boat. [SEP]\n",
      "Base  Answer: a balding man wearing a red life jacket sitting in a small boat. [SEP]\n",
      "\n",
      "Example 5\n",
      "Input: [917, 946, 50, 297, 14737, 26911, 22828, 26918, 30, 155, 5146, 147, 9959, 16739, 1364, 4777, 1065, 50, 297, 18903, 8993, 4429, 3330, 26918, 15284, 2768, 142, 303, 3103, 8670, 2534, 50, 30, 2116, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
      "ExpandAnswer: a woman in a red coat holding a purse in a blue purse, jumping into a blue flowered, while holding a female in a string. [SEP]\n",
      "Base  Answer: a woman in a red coat holding a purse in a blue purse is jumping from a blue and holding a paintbrush in the air. [SEP]\n",
      "\n",
      "Example 6\n",
      "Input: [198, 20869, 6, 9217, 8834, 4053, 128, 10400, 9217, 18485, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A brown dog is running after the black dog.\n",
      "ExpandAnswer: a brown dog is running with black dog. [SEP]\n",
      "Base  Answer: a brown dog runs after the black dog. [SEP]\n",
      "\n",
      "Example 7\n",
      "Input: [198, 5934, 10828, 114, 297, 3945, 15288, 26935, 22321, 1789, 2768, 303, 22829, 5391, 26900, 50, 3134, 443, 24654, 65, 4106, 26902, 26914, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
      "ExpandAnswer: a little boy wearing a baseball jersey swings a baseball ball towards a ball. [SEP]\n",
      "Base  Answer: a little boy with a bat swinging a bat in a ball bat towards a ball. [SEP]\n",
      "\n",
      "Example 8\n",
      "Input: [198, 980, 15826, 321, 50, 297, 16429, 139, 7949, 7, 7173, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A man in a cluttered office is using the telephone\n",
      "ExpandAnswer: a man talks on the phone in an office setting. [SEP]\n",
      "Base  Answer: a man is talking on a phone in an office. [SEP]\n",
      "\n",
      "Example 9\n",
      "Input: [917, 1567, 343, 26907, 21748, 946, 114, 297, 6699, 24162, 20, 9790, 7, 6129, 3653, 9614, 3330, 39, 20329, 5475, 772, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "GoldenAnswer: A smiling woman in a peach tank top stands holding a mountain bike\n",
      "ExpandAnswer: a smiling woman with a peach and a mountain bike holds a mountain bike. [SEP]\n",
      "Base  Answer: a smiling woman with a peach tank top holds a mountain bike. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import Transformer_Base\n",
    "import Transformer_Expand\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = tokenizer_src.vocab_size\n",
    "tgt_vocab_size = tokenizer_tgt.vocab_size\n",
    "d_model = 768  # BERT의 경우\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "# 모델 초기화\n",
    "transformer = Transformer_Expand.Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "model_state_dict = torch.load('../../parameters/scratch/base_expand_de_en_checkpoint_epoch_5.pth')\n",
    "transformer.load_state_dict(model_state_dict['model_state_dict'])\n",
    "# 모델 초기화\n",
    "transformer2 = Transformer_Base.Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "model_state_dict = torch.load('../../parameters/scratch/base_attention_de_en_checkpoint_epoch_5.pth')\n",
    "transformer2.load_state_dict(model_state_dict['model_state_dict'])\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.pad_token_id)\n",
    "def predict(model, input_sequence, max_length=128, SOS_token=101, EOS_token=102):\n",
    "    model.eval()\n",
    "    # Ensure input_sequence is a tensor and reshape if necessary\n",
    "    if not isinstance(input_sequence, torch.Tensor):\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.long)\n",
    "    input_sequence = input_sequence.unsqueeze(0)  # Add batch dimension if missing\n",
    "    input_sequence = input_sequence.to(device)  # Move to the same device as the model\n",
    "\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            pred = model(input_sequence, y_input)\n",
    "            next_item = pred[:, -1, :].topk(1)[1].view(-1).item()  # Get the last word's top prediction\n",
    "            next_item = torch.tensor([[next_item]], device=device)\n",
    "            #print(next_item)\n",
    "            y_input = torch.cat((y_input, next_item), dim=1)\n",
    "            if next_item.view(-1).item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    " \n",
    "val_dataloader.dataset['labels'][1]\n",
    "# Here we test some examples to observe how the model predicts\n",
    "\n",
    "examples2 = test_dataloader.dataset['input_ids'][:10]\n",
    " \n",
    "for idx, example in enumerate(examples2):\n",
    "    result = predict(transformer, example)\n",
    "    result2 = predict(transformer2, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example[1:-1]}\")\n",
    "    print(f\"GoldenAnswer: {test_dataloader.dataset['en'][idx]}\")\n",
    "    print(f\"ExpandAnswer: {tokenizer_tgt.decode(result[1:])}\")\n",
    "    print(f\"Base  Answer: {tokenizer_tgt.decode(result2[1:])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "env01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
